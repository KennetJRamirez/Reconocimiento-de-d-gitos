{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KennetJRamirez/Reconocimiento-de-d-gitos/blob/main/2_Reconocimiento_de_d%C3%ADgitos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9FHxrmxnDel"
      },
      "source": [
        "<h1><font color=\"#113D68\" size=6>Deep Learning con Python y Keras</font></h1>\n",
        "\n",
        "<h1><font color=\"#113D68\" size=5>Parte 5. Redes Neuronales Convolucionales</font></h1>\n",
        "\n",
        "<h1><font color=\"#113D68\" size=4>2. Reconocimiento de dígitos</font></h1>\n",
        "\n",
        "<br><br>\n",
        "<div style=\"text-align: right\">\n",
        "<font color=\"#113D68\" size=3>Manuel Castillo Cara</font><br>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyVOhqK8nDev"
      },
      "source": [
        "---\n",
        "\n",
        "<a id=\"indice\"></a>\n",
        "<h2><font color=\"#004D7F\" size=5>Índice</font></h2>\n",
        "\n",
        "* [0. Contexto](#section0)\n",
        "* [1. MNIST dataset](#section1)\n",
        "* [2. Cargar MNIST](#section2)\n",
        "* [3. Modelo de línea de base con MLP](#section3)\n",
        "* [4. CNN para MNIST](#section4)\n",
        "* [5. CNN más profunda para MNIST](#section5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xUEES82nDew"
      },
      "source": [
        "---\n",
        "<a id=\"section0\"></a>\n",
        "# <font color=\"#004D7F\" size=6> 0. Contexto</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwjeagCWnDex"
      },
      "source": [
        "En este proyecto, descubrirá cómo desarrollar un modelo de Deep Learning en la tarea de reconocimiento de dígitos manuscritos del MNIST. Después de completar esta clase sabrá:\n",
        "* Cómo cargar MNIST y desarrollar un modelo de red neuronal.\n",
        "* Cómo implementar y evaluar una CNN de línea base para MNIST.\n",
        "* Cómo implementar un modelo de Deep Learning avanzado para MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXTSz97onDex"
      },
      "source": [
        "import tensorflow as tf\n",
        "# Eliminar warning\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJztWiOjnDex"
      },
      "source": [
        "---\n",
        "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj1BtYifnDey"
      },
      "source": [
        "<a id=\"section1\"></a>\n",
        "# <font color=\"#004D7F\" size=6>1. MNIST dataset</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmgCoRlMnDey"
      },
      "source": [
        "MNIST toma imágenes de dígitos de una variedad de documentos escaneados, normalizados en tamaño y centrados.\n",
        "\n",
        "Cada imagen es está dada en blanco y negro con $28 × 28$ píxeles (784 píxeles en total). Se usan 60,000 imágenes para entrenar un modelo y 10,000 imágenes para validarlo.\n",
        "\n",
        "Es una tarea de reconocimiento de dígitos. Como tal, hay 10 dígitos (0 a 9) o 10 clases para predecir.\n",
        "\n",
        "En la página web de Rodrigo Benenson hay una lista de los resultados más avanzados y enlaces a los artículos relevantes sobre el MNIST y otros conjuntos de datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk7IGobWnDey"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
        "Más información sobre el dataset [MNIST](http://yann.lecun.com/exdb/mnist/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOOJKuKtnDez"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
        "Información de los resultados sobre MNIST de [Rodrigo Benenson](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBRF--h3nDez"
      },
      "source": [
        "---\n",
        "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25YqTpeVnDez"
      },
      "source": [
        "<a id=\"section2\"></a>\n",
        "# <font color=\"#004D7F\" size=6>2. Cargar MNIST</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5kaqoWtnDez"
      },
      "source": [
        "El conjunto de datos se descarga automáticamente la primera vez que se llama a esta función y se almacena en su directorio de inicio en `~/.keras/datasets/mnist.pkl.gz` como un archivo de 15 megabytes.\n",
        "\n",
        "Primero escribiremos un pequeño script para descargar y visualizar las primeras 4 imágenes mediante la función `mnist.load data()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "ZM4vwwl1nDe0",
        "outputId": "d0a15e89-da38-4fbb-f53a-0523bef9462a"
      },
      "source": [
        "# Plot ad hoc mnist instances\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# load (downloaded if needed) the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# plot 4 images as gray scale\n",
        "plt.subplot(221)\n",
        "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(222)\n",
        "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(223)\n",
        "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(224)\n",
        "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeUAAAGfCAYAAABhicrFAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAMXJJREFUeJzt3Xt0lPWdx/FPgmS4mAyGS0JKwKgIXSlhFwlGKKKmhFgtt+2qx1WoHvESOAL1cuJB8FajYK3FIrq1glQRy7GBSrdYGiAcV6ASoBxEssBSCUKCsptJiBAw+e0fHKZGfmMyyQzzm5n365znHPPJk2e+Dybfb57Mb55JMMYYAQCAiEuMdAEAAOAMhjIAAI5gKAMA4AiGMgAAjmAoAwDgCIYyAACOYCgDAOAIhjIAAI5gKAMA4AiGMgAAjrggXAdeuHCh5s+fr6qqKmVnZ+ull15STk5Oi1/X1NSkw4cPKzk5WQkJCeEqD2gTY4zq6uqUkZGhxER+pw21tvYNid4Bt7W6d5gwWL58uUlKSjKvv/66+fjjj83dd99tunXrZqqrq1v82srKSiOJjc3prbKyMhw/OnGtPX3DGHoHW3RsLfWOsAzlnJwcU1hY6P+4sbHRZGRkmOLi4ha/tqamJuL/aGxsLW01NTXh+NGJa+3pG8bQO9iiY2upd4T872+nTp1SeXm58vLy/FliYqLy8vK0adOmc/ZvaGhQbW2tf6urqwt1SUDI8efR0Aq2b0j0DkSnlnpHyIfyF198ocbGRqWlpTXL09LSVFVVdc7+xcXF8nq9/i0zMzPUJQFwXLB9Q6J3IDZFfKVKUVGRfD6ff6usrIx0SQCiAL0DsSjkq6979OihDh06qLq6ulleXV2t9PT0c/b3eDzyeDyhLgNAFAm2b0j0DsSmkF8pJyUlaejQoSotLfVnTU1NKi0tVW5ubqgfDkAMoG8AZ4TldcqzZs3S5MmTdeWVVyonJ0cvvvii6uvr9ZOf/CQcDwcgBtA3gDAN5Ztvvlmff/655syZo6qqKg0ZMkRr1qw5ZxEHAJxF3wCkBGOMiXQRX1dbWyuv1xvpMoBv5fP5lJKSEuky8DX0DkSDlnpHxFdfAwCAMxjKAAA4gqEMAIAjGMoAADiCoQwAgCMYygAAOIKhDACAIxjKAAA4gqEMAIAjGMoAADiCoQwAgCMYygAAOIKhDACAIxjKAAA4IizvpwwAiF1Dhw615tOmTbPmd9xxhzVfunSpNX/ppZes+bZt21pRXXTjShkAAEcwlAEAcARDGQAARzCUAQBwRMiH8uOPP66EhIRm28CBA0P9MABiCH0DOCMsq6+vuOIK/eUvf/nHg1zAIu9Q6tChgzX3er0hOX6gFZRdunSx5gMGDLDmhYWF1vz555+35rfeeqs1P3nypDV/9tlnrfkTTzxhzeE2+oZ7hgwZYs3Xrl1rzVNSUqy5Mcaa33777db8Rz/6kTXv3r27NY8lYfmuv+CCC5Senh6OQwOIUfQNIEzPKe/du1cZGRm65JJLdNttt+ngwYMB921oaFBtbW2zDUD8CaZvSPQOxKaQD+Xhw4dryZIlWrNmjRYtWqQDBw7o+9//vurq6qz7FxcXy+v1+rfMzMxQlwTAccH2DYnegdgU8qFcUFCgH//4xxo8eLDy8/P1n//5n6qpqdHvfvc76/5FRUXy+Xz+rbKyMtQlAXBcsH1DoncgNoV9JUW3bt10+eWXa9++fdbPezweeTyecJcBIIq01DckegdiU9iH8vHjx7V///6Aq+xiUd++fa15UlKSNb/66qut+ciRI615t27drPmkSZNaLi4MDh06ZM0XLFhgzSdMmGDNA/2p8m9/+5s1Lysra0V1iEbx2DciKScnx5q/++671jzQKz0CrbIO9LN96tQpax5olfVVV11lzQPdEzvQ8V0W8j9fP/jggyorK9Pf//53ffjhh5owYYI6dOgQ8OUuAEDfAM4I+ZXyoUOHdOutt+rYsWPq2bOnRo4cqc2bN6tnz56hfigAMYK+AZwR8qG8fPnyUB8SQIyjbwBncO9rAAAcwVAGAMARCSbQcrkIqa2tDdk9nMMt0H1h161bZ82j5bwCaWpqsuZ33nmnNT9+/HhQxz9y5Ig1/7//+z9rXlFREdTxQ8nn8wW8zy8iI5p6R7gFuk/9v/zLv1jzN99805r36dPHmickJFjzQOMk0OroefPmWfNAT2cEetzZs2db8+LiYmseSS31Dq6UAQBwBEMZAABHMJQBAHAEQxkAAEcwlAEAcETY730dywK93+uxY8eseaRWhm7ZssWa19TUWPNrr73Wmge6j+xvf/vbNtUFIDxeffVVax6p25YGWvV94YUXWvNA97UfPXq0NR88eHCb6nIRV8oAADiCoQwAgCMYygAAOIKhDACAIxjKAAA4gtXX7fC///u/1vyhhx6y5jfeeKM13759uzVfsGBBUPXs2LHDmv/gBz+w5vX19db8iiuusOYPPPBAUPUACK+hQ4da8x/+8IfWPNC9owMJtAr6vffes+bPP/+8NT98+LA1D9T7At3v/rrrrrPmwZ6Xy7hSBgDAEQxlAAAcwVAGAMARDGUAABwR9FDeuHGjbrrpJmVkZCghIUErV65s9nljjObMmaPevXurc+fOysvL0969e0NVL4AoRN8AWifo1df19fXKzs7WnXfeqYkTJ57z+Xnz5mnBggV64403lJWVpccee0z5+fnavXu3OnXqFJKiXffNhnPWunXrrHldXZ01z87OtuZ33XWXNQ+08jHQKutAPv74Y2s+derUoI4DnEXfaJ8hQ4ZY87Vr11rzlJQUa26MseZ/+tOfrHmge2Vfc8011nz27NnW/LXXXrPmn3/+uTX/29/+Zs2bmpqseaDV5oHuub1t2zZr7oKgh3JBQYEKCgqsnzPG6MUXX9Ts2bM1btw4SdLSpUuVlpamlStX6pZbbmlftQCiEn0DaJ2QPqd84MABVVVVKS8vz595vV4NHz5cmzZtsn5NQ0ODamtrm20A4kdb+oZE70BsCulQrqqqkiSlpaU1y9PS0vyf+6bi4mJ5vV7/lpmZGcqSADiuLX1DoncgNkV89XVRUZF8Pp9/q6ysjHRJAKIAvQOxKKRDOT09XZJUXV3dLK+urvZ/7ps8Ho9SUlKabQDiR1v6hkTvQGwK6b2vs7KylJ6ertLSUv9qwdraWm3ZskX33XdfKB8qKgX7nJfP5wtq/7vvvtuav/POO9Y80EpG4Hyib/zD5Zdfbs0D3U/f6/Va8y+++MKaHzlyxJq/8cYb1vz48ePW/I9//GNQebh17tzZmv/0pz+15rfddls4y2mXoIfy8ePHtW/fPv/HBw4c0I4dO5Samqq+fftqxowZevrpp9W/f3//SxsyMjI0fvz4UNYNIIrQN4DWCXoob926Vddee63/41mzZkmSJk+erCVLlujhhx9WfX29pk6dqpqaGo0cOVJr1qzhtYZAHKNvAK0T9FAePXp0wBegS2feQuvJJ5/Uk08+2a7CAMQO+gbQOhFffQ0AAM5gKAMA4IiQrr5GaD3++OPWfOjQodY80P1ov36npK/785//3Ka6ALSPx+Ox5oHuX3/DDTdY80D3zb/jjjus+datW615oNXL0a5v376RLiFoXCkDAOAIhjIAAI5gKAMA4AiGMgAAjmAoAwDgCFZfO6y+vt6aB7rH9bZt26z5r3/9a2u+fv16ax5ohebChQut+bfdFALAuf75n//ZmgdaZR3IuHHjrHlZWVnQNcENXCkDAOAIhjIAAI5gKAMA4AiGMgAAjmAoAwDgCFZfR6H9+/db8ylTpljzxYsXW/Pbb789qLxr167WfOnSpdb8yJEj1hyIdy+88II1T0hIsOaBVlPH2yrrxET7dWRTU9N5riR8uFIGAMARDGUAABzBUAYAwBEMZQAAHBH0UN64caNuuukmZWRkKCEhQStXrmz2+SlTpighIaHZNnbs2FDVCyAK0TeA1gl69XV9fb2ys7N15513auLEidZ9xo4d22zFr8fjaXuFaLWSkhJrvnfvXmseaAXo9ddfb82feeYZa96vXz9r/rOf/cyaf/bZZ9YcsSte+8aNN95ozYcMGWLNA91H/g9/+EOoSopqgVZZB/p327FjRxirCY+gh3JBQYEKCgq+dR+Px6P09PQ2FwUgttA3gNYJy3PKGzZsUK9evTRgwADdd999OnbsWMB9GxoaVFtb22wDEH+C6RsSvQOxKeRDeezYsVq6dKlKS0v13HPPqaysTAUFBWpsbLTuX1xcLK/X698yMzNDXRIAxwXbNyR6B2JTyO/odcstt/j/+3vf+54GDx6sSy+9VBs2bLA+V1lUVKRZs2b5P66treWHC4gzwfYNid6B2BT2l0Rdcskl6tGjh/bt22f9vMfjUUpKSrMNQHxrqW9I9A7EprDf+/rQoUM6duyYevfuHe6HQgC7du2y5v/2b/9mzW+66SZrHuge2vfcc48179+/vzX/wQ9+YM2Bs2Klb3Tu3NmaJyUlWfOjR49a83feeSdkNbkk0Ar7xx9/PKjjrFu3zpoXFRUFW1LEBT2Ujx8/3uy31wMHDmjHjh1KTU1VamqqnnjiCU2aNEnp6enav3+/Hn74YV122WXKz88PaeEAogd9A2idoIfy1q1bde211/o/PvuczuTJk7Vo0SLt3LlTb7zxhmpqapSRkaExY8boqaeeionXHAJoG/oG0DpBD+XRo0cHfKG2JL3//vvtKghA7KFvAK3Dva8BAHAEQxkAAEeEffU13FVTU2PNf/vb31rz1157zZpfcIH922jUqFHWfPTo0dZ8w4YN1hyIFw0NDdb8yJEj57mS0Aq0NmD27NnW/KGHHrLmhw4dsuY///nPrfnx48dbUZ1buFIGAMARDGUAABzBUAYAwBEMZQAAHMFQBgDAEay+jgODBw+25v/6r/9qzYcNG2bNA62yDmT37t3WfOPGjUEdB4gXf/jDHyJdQrsMGTLEmgdaTX3zzTdb81WrVlnzSZMmtamuaMKVMgAAjmAoAwDgCIYyAACOYCgDAOAIhjIAAI5g9XUUGjBggDWfNm2aNZ84caI1T09PD0k9jY2N1jzQ/XqbmppC8riA6xISEoLKx48fb80feOCBUJUUEjNnzrTmjz32mDX3er3W/K233rLmd9xxR9sKiwFcKQMA4AiGMgAAjmAoAwDgCIYyAACOCGooFxcXa9iwYUpOTlavXr00fvx4VVRUNNvn5MmTKiwsVPfu3XXhhRdq0qRJqq6uDmnRAKILvQNonaBWX5eVlamwsFDDhg3TV199pUcffVRjxozR7t271bVrV0lnVuX98Y9/1IoVK+T1ejVt2jRNnDhR//Vf/xWWE4gFgVZB33rrrdY80Crriy++OFQlWW3dutWa/+xnP7Pm0X4fX4ROvPYOY0xQeaBesGDBAmv++uuvW/Njx45Z86uuusqa33777dY8Ozvbmvfp08eaHzx40Jq///771vzll1+25vEsqKG8Zs2aZh8vWbJEvXr1Unl5uUaNGiWfz6ff/OY3WrZsma677jpJ0uLFi/Xd735XmzdvDvgNASC20TuA1mnXc8o+n0+SlJqaKkkqLy/X6dOnlZeX599n4MCB6tu3rzZt2mQ9RkNDg2pra5ttAGIbvQOwa/NQbmpq0owZMzRixAgNGjRIklRVVaWkpCR169at2b5paWmqqqqyHqe4uFher9e/ZWZmtrUkAFGA3gEE1uahXFhYqF27dmn58uXtKqCoqEg+n8+/VVZWtut4ANxG7wACa9NtNqdNm6bVq1dr48aNzZ7wT09P16lTp1RTU9PsN97q6uqACxg8Ho88Hk9bygAQZegdwLcLaigbYzR9+nSVlJRow4YNysrKavb5oUOHqmPHjiotLdWkSZMkSRUVFTp48KByc3NDV7Xj0tLSrPk//dM/WfNf/epX1nzgwIEhq8lmy5Yt1nz+/PnWfNWqVdace1mjJfSO1unQoYM1v//++6352X+rbwr0/Hr//v3bVtg3fPjhh9Z8/fr11nzOnDkhedx4ENRQLiws1LJly7Rq1SolJyf7n+vxer3q3LmzvF6v7rrrLs2aNUupqalKSUnR9OnTlZuby+pJII7RO4DWCWooL1q0SJI0evToZvnixYs1ZcoUSdIvfvELJSYmatKkSWpoaFB+fj6vRQPiHL0DaJ2g/3zdkk6dOmnhwoVauHBhm4sCEFvoHUDrcO9rAAAcwVAGAMARbXpJVLw5e9ehb3r11Vet+ZAhQ6z5JZdcEqqSrAKtiPz5z39uzQPdj/bEiRMhqwmIZ4HuRvbRRx9Z82HDhgV1/EAvFwv0CpBAAt0rO9BryR944IGgjo/W40oZAABHMJQBAHAEQxkAAEcwlAEAcARDGQAAR8Tl6uvhw4db84ceesia5+TkWPPvfOc7IavJ5ssvv7TmCxYssObPPPOMNa+vrw9ZTQBa79ChQ9Z84sSJ1vyee+6x5rNnzw5JPb/85S+t+dk7rn3Tvn37QvK4aD2ulAEAcARDGQAARzCUAQBwBEMZAABHMJQBAHBEgmnNe6qdR7W1tfJ6vWF9jGeffdaaB1p9Hazdu3db89WrV1vzr776ypoHumd1TU1Nm+pC6Ph8PqWkpES6DHzN+egdQHu11Du4UgYAwBEMZQAAHMFQBgDAEQxlAAAcEdRQLi4u1rBhw5ScnKxevXpp/PjxqqioaLbP6NGjlZCQ0Gy79957Q1o0gOhC7wBaJ6jV12PHjtUtt9yiYcOG6auvvtKjjz6qXbt2affu3erataukMz9Yl19+uZ588kn/13Xp0qXVK1VZQYlowOrr4NA7gDNa6h1BvSHFmjVrmn28ZMkS9erVS+Xl5Ro1apQ/79Kli9LT04MsFUCsoncArdOu55R9Pp8kKTU1tVn+1ltvqUePHho0aJCKiooCvtuRJDU0NKi2trbZBiC20TuAAEwbNTY2mh/+8IdmxIgRzfJXX33VrFmzxuzcudO8+eab5jvf+Y6ZMGFCwOPMnTvXSGJji6rN5/O19Ucn7tE72OJ5a6l3tHko33vvvaZfv36msrLyW/crLS01ksy+ffusnz958qTx+Xz+rbKyMuL/aGxsLW0M5bajd7DF89ZS7wjqOeWzpk2bptWrV2vjxo3q06fPt+47fPhwSWfeLPvSSy895/Mej0cej6ctZQCIMvQO4NsFNZSNMZo+fbpKSkq0YcMGZWVltfg1O3bskCT17t27TQUCiH70DqB1ghrKhYWFWrZsmVatWqXk5GRVVVVJkrxerzp37qz9+/dr2bJluuGGG9S9e3ft3LlTM2fO1KhRozR48OCwnAAA99E7gFYK5rkgBfgb+eLFi40xxhw8eNCMGjXKpKamGo/HYy677DLz0EMPBfX8m8/ni/jf/NnYWtp4Tjk4gf4d6R1s8ba19D0dl2/dCLQXNw9xD70D0YC3bgQAIEowlAEAcARDGQAARzCUAQBwBEMZAABHMJQBAHAEQxkAAEc4N5Qde9k0YMX3qXv4f4Jo0NL3qXNDua6uLtIlAC3i+9Q9/D9BNGjp+9S5O3o1NTXp8OHDSk5OVl1dnTIzM1VZWRkXd0+qra3lfB1njFFdXZ0yMjKUmOjc77Rxjd7B+bqstb2jTW/dGE6JiYn+t3RLSEiQJKWkpETNP3wocL5u41aObqJ3cL6ua03v4Fd9AAAcwVAGAMARTg9lj8ejuXPnyuPxRLqU84LzBUIj3r63ON/Y4dxCLwAA4pXTV8oAAMQThjIAAI5gKAMA4AiGMgAAjnB6KC9cuFAXX3yxOnXqpOHDh+uvf/1rpEsKiY0bN+qmm25SRkaGEhIStHLlymafN8Zozpw56t27tzp37qy8vDzt3bs3MsWGQHFxsYYNG6bk5GT16tVL48ePV0VFRbN9Tp48qcLCQnXv3l0XXnihJk2apOrq6ghVjGgWq31Diq/eEa99w9mh/M4772jWrFmaO3eutm3bpuzsbOXn5+vo0aORLq3d6uvrlZ2drYULF1o/P2/ePC1YsECvvPKKtmzZoq5duyo/P18nT548z5WGRllZmQoLC7V582atXbtWp0+f1pgxY1RfX+/fZ+bMmXrvvfe0YsUKlZWV6fDhw5o4cWIEq0Y0iuW+IcVX74jbvmEclZOTYwoLC/0fNzY2moyMDFNcXBzBqkJPkikpKfF/3NTUZNLT0838+fP9WU1NjfF4PObtt9+OQIWhd/ToUSPJlJWVGWPOnF/Hjh3NihUr/Pt88sknRpLZtGlTpMpEFIqXvmFM/PWOeOkbTl4pnzp1SuXl5crLy/NniYmJysvL06ZNmyJYWfgdOHBAVVVVzc7d6/Vq+PDhMXPuPp9PkpSamipJKi8v1+nTp5ud88CBA9W3b9+YOWeEXzz3DSn2e0e89A0nh/IXX3yhxsZGpaWlNcvT0tJUVVUVoarOj7PnF6vn3tTUpBkzZmjEiBEaNGiQpDPnnJSUpG7dujXbN1bOGedHPPcNKbZ7Rzz1DefeJQqxrbCwULt27dIHH3wQ6VIARIl46htOXin36NFDHTp0OGcVXXV1tdLT0yNU1flx9vxi8dynTZum1atXa/369f632JPOnPOpU6dUU1PTbP9YOGecP/HcN6TY7R3x1jecHMpJSUkaOnSoSktL/VlTU5NKS0uVm5sbwcrCLysrS+np6c3Ovba2Vlu2bInaczfGaNq0aSopKdG6deuUlZXV7PNDhw5Vx44dm51zRUWFDh48GLXnjPMvnvuGFHu9I277RqRXmgWyfPly4/F4zJIlS8zu3bvN1KlTTbdu3UxVVVWkS2u3uro6s337drN9+3Yjybzwwgtm+/bt5tNPPzXGGPPss8+abt26mVWrVpmdO3eacePGmaysLHPixIkIV9429913n/F6vWbDhg3myJEj/u3LL7/073Pvvfeavn37mnXr1pmtW7ea3Nxck5ubG8GqEY1iuW8YE1+9I177hrND2RhjXnrpJdO3b1+TlJRkcnJyzObNmyNdUkisX7/eSDpnmzx5sjHmzEsbHnvsMZOWlmY8Ho+5/vrrTUVFRWSLbgfbuUoyixcv9u9z4sQJc//995uLLrrIdOnSxUyYMMEcOXIkckUjasVq3zAmvnpHvPYN3roRAABHOPmcMgAA8YihDACAIxjKAAA4gqEMAIAjGMoAADiCoQwAgCMYygAAOIKhDACAIxjKAAA4gqEMAIAjnHs/5aamJh0+fFjJyclKSEiIdDlAM8YY1dXVKSMjQ4mJ/E7rEnoHXNbq3hGum2r/6le/Mv369TMej8fk5OSYLVu2tOrrKisrA96InI3Nla2ysjJcPzpxra19wxh6B1t0bC31jrD8qv/OO+9o1qxZmjt3rrZt26bs7Gzl5+fr6NGjLX5tcnJyOEoCQorv09BrT9+Q+H+C6NDi92l7f7O1ycnJMYWFhf6PGxsbTUZGhikuLj5n35MnTxqfz+ff+G2XLRo2n88Xjh+duBZM3zCG3sEWnVtLvSPkV8qnTp1SeXm58vLy/FliYqLy8vK0adOmc/YvLi6W1+v1b5mZmaEuCYDjgu0bEr0DsSnkQ/mLL75QY2Oj0tLSmuVpaWmqqqo6Z/+ioiL5fD7/VllZGeqSADgu2L4h0TsQmyK++trj8cjj8US6DABRht6BWBTyK+UePXqoQ4cOqq6ubpZXV1crPT091A8HIAbQN4AzQj6Uk5KSNHToUJWWlvqzpqYmlZaWKjc3N9QPByAG0DeAM8Ly5+tZs2Zp8uTJuvLKK5WTk6MXX3xR9fX1+slPfhKOhwMQA+gbQJiG8s0336zPP/9cc+bMUVVVlYYMGaI1a9acs4gDAM6ibwBSgjHGRLqIr6utrZXX6410GcC38vl8SklJiXQZ+Bp6B6JBS72Dm/cCAOAIhjIAAI5gKAMA4AiGMgAAjmAoAwDgCIYyAACOYCgDAOAIhjIAAI5gKAMA4AiGMgAAjmAoAwDgCIYyAACOYCgDAOAIhjIAAI5gKAMA4AiGMgAAjmAoAwDgCIYyAACOYCgDAOCIC0J9wMcff1xPPPFEs2zAgAHas2dPqB8KUez666+35m+99ZY1v+aaa6x5RUVFyGpC5NA34tPs2bOt+Te/F85KTLRfR44ePdqal5WVtamuSAr5UJakK664Qn/5y1/+8SAXhOVhAMQQ+gYQpqF8wQUXKD09vVX7NjQ0qKGhwf9xbW1tOEoC4Lhg+oZE70BsCstzynv37lVGRoYuueQS3XbbbTp48GDAfYuLi+X1ev1bZmZmOEoC4Lhg+oZE70BsCvlQHj58uJYsWaI1a9Zo0aJFOnDggL7//e+rrq7Oun9RUZF8Pp9/q6ysDHVJABwXbN+Q6B2ITSH/83VBQYH/vwcPHqzhw4erX79++t3vfqe77rrrnP09Ho88Hk+oywAQRYLtGxK9A7Ep7CspunXrpssvv1z79u0L90O12qhRo6x59+7drXlJSUk4y4lLw4YNs+YfffTRea4ELnKxb6DtpkyZYs0feeQRa97U1BTU8Y0xwZbkrLC/Tvn48ePav3+/evfuHe6HAhAj6BuIVyEfyg8++KDKysr097//XR9++KEmTJigDh066NZbbw31QwGIEfQN4IyQ//n60KFDuvXWW3Xs2DH17NlTI0eO1ObNm9WzZ89QPxSAGEHfAM4I+VBevnx5qA8JIMbRN4AzuPc1AACOiMv72AW6T2r//v2tOauv2y7QvWqzsrKseb9+/ax5QkJCyGoCcH4F+rnu1KnTea7EfVwpAwDgCIYyAACOYCgDAOAIhjIAAI5gKAMA4Ii4XH19xx13WPNNmzad50piX6DbJN59993W/M0337Tme/bsCVlNAMIjLy/Pmk+fPj2o4wT6eb/xxhuteXV1dVDHdxlXygAAOIKhDACAIxjKAAA4gqEMAIAjGMoAADgiLldfB7ofM0LvtddeC2r/vXv3hqkSAKEycuRIa7548WJr7vV6gzr+/Pnzrfmnn34a1HGiEdMJAABHMJQBAHAEQxkAAEcwlAEAcARDGQAARwS9+nrjxo2aP3++ysvLdeTIEZWUlGj8+PH+zxtjNHfuXP36179WTU2NRowYoUWLFql///6hrLtVBg8ebM3T0tLOcyXxK9hVl2vXrg1TJYikaOobaNnkyZOteUZGRlDH2bBhgzVfunRpsCXFjKCvlOvr65Wdna2FCxdaPz9v3jwtWLBAr7zyirZs2aKuXbsqPz9fJ0+ebHexAKITfQNonaCvlAsKClRQUGD9nDFGL774ombPnq1x48ZJOvMbT1pamlauXKlbbrnlnK9paGhQQ0OD/+Pa2tpgSwLguFD3DYnegdgU0ueUDxw4oKqqqmZv3+X1ejV8+PCAb4tYXFwsr9fr3zIzM0NZEgDHtaVvSPQOxKaQDuWqqipJ5z5nm5aW5v/cNxUVFcnn8/m3ysrKUJYEwHFt6RsSvQOxKeK32fR4PPJ4PJEuA0CUoXcgFoV0KKenp0uSqqur1bt3b39eXV2tIUOGhPKhWuWGG26w5p07dz7PlcS+QCvas7KygjrOZ599FopyEEVc6xv4hx49eljzO++805o3NTVZ85qaGmv+9NNPt6muWBbSP19nZWUpPT1dpaWl/qy2tlZbtmxRbm5uKB8KQIygbwD/EPSV8vHjx7Vv3z7/xwcOHNCOHTuUmpqqvn37asaMGXr66afVv39/ZWVl6bHHHlNGRkaz1yQCiC/0DaB1gh7KW7du1bXXXuv/eNasWZLOvJh8yZIlevjhh1VfX6+pU6eqpqZGI0eO1Jo1a9SpU6fQVQ0gqtA3gNYJeiiPHj1axpiAn09ISNCTTz6pJ598sl2FAYgd9A2gdbj3NQAAjoj4S6LCacCAAUHt//HHH4epktj3/PPPW/NAq7L/+7//25rX1dWFrCYArXPxxRdb83fffTckx3/ppZes+fr160Ny/FjClTIAAI5gKAMA4AiGMgAAjmAoAwDgCIYyAACOiOnV18H66KOPIl3CeZeSkmLNx44da83//d//3ZqPGTMmqMd96qmnrHmge+QCCJ9AP++DBw8O6jhfv1Xq1/3yl78MuqZ4xZUyAACOYCgDAOAIhjIAAI5gKAMA4AiGMgAAjmD19dekpqaG9fjZ2dnWPCEhwZrn5eVZ8z59+ljzpKQka37bbbcFrCkx0f572YkTJ6z5li1brHlDQ4M1v+AC+7dYeXl5wJoAhEeg96d+9tlngzrOBx98YM0nT55szX0+X1DHj2dcKQMA4AiGMgAAjmAoAwDgCIYyAACOYCgDAOCIoFdfb9y4UfPnz1d5ebmOHDmikpKSZiv6pkyZojfeeKPZ1+Tn52vNmjXtLjZYgVYQG2Os+SuvvGLNH3300ZDUE+g+soFWX3/11VfW/Msvv7Tmu3fvtuavv/56wJq2bt1qzcvKyqx5dXW1NT906JA179y5szXfs2dPwJoQe6Kpb8SCiy++2Jq/++67ITn+//zP/1jzQP0BrRf0lXJ9fb2ys7O1cOHCgPuMHTtWR44c8W9vv/12u4oEEN3oG0DrBH2lXFBQoIKCgm/dx+PxKD09vVXHa2hoaPYa19ra2mBLAuC4UPcNid6B2BSW55Q3bNigXr16acCAAbrvvvt07NixgPsWFxfL6/X6t8zMzHCUBMBxwfQNid6B2BTyoTx27FgtXbpUpaWleu6551RWVqaCggI1NjZa9y8qKpLP5/NvlZWVoS4JgOOC7RsSvQOxKeS32bzlllv8//29731PgwcP1qWXXqoNGzbo+uuvP2d/j8cjj8cT6jIARJFg+4ZE70BsCvu9ry+55BL16NFD+/btC/jDFS7333+/Nf/000+t+dVXXx3OcnTw4EFrvnLlSmv+ySefWPPNmzeHqqSgTZ061Zr37NnTmgdapQl8m0j2jVjwyCOPWPOmpqaQHD/Ye2Wj9cL+OuVDhw7p2LFj6t27d7gfCkCMoG8gXgV9pXz8+HHt27fP//GBAwe0Y8cOpaamKjU1VU888YQmTZqk9PR07d+/Xw8//LAuu+wy5efnh7RwANGDvgG0TtBDeevWrbr22mv9H8+aNUvSmbfsWrRokXbu3Kk33nhDNTU1ysjI0JgxY/TUU0/x3A8Qx+gbQOsEPZRHjx4d8I5YkvT++++3qyAAsYe+AbQO974GAMARYV997aLnnnsu0iVErWBXwobqXrsAzjVkyBBrPmbMmJAcf9WqVda8oqIiJMfHubhSBgDAEQxlAAAcwVAGAMARDGUAABzBUAYAwBFxufoa509JSUmkSwBi1p///GdrftFFFwV1nED3058yZUqwJaGduFIGAMARDGUAABzBUAYAwBEMZQAAHMFQBgDAEay+BoAo1b17d2ve1NQU1HFefvlla378+PGga0L7cKUMAIAjGMoAADiCoQwAgCMYygAAOIKhDACAI4JafV1cXKzf//732rNnjzp37qyrr75azz33nAYMGODf5+TJk/rpT3+q5cuXq6GhQfn5+Xr55ZeVlpYW8uLhjoSEBGt++eWXW/NA99pFbKJ3tM/ixYuteWJiaK6rPvzww5AcB+0X1P/RsrIyFRYWavPmzVq7dq1Onz6tMWPGqL6+3r/PzJkz9d5772nFihUqKyvT4cOHNXHixJAXDiB60DuA1gnqSnnNmjXNPl6yZIl69eql8vJyjRo1Sj6fT7/5zW+0bNkyXXfddZLO/Ib33e9+V5s3b9ZVV111zjEbGhrU0NDg/7i2trYt5wHAYfQOoHXa9bcPn88nSUpNTZUklZeX6/Tp08rLy/PvM3DgQPXt21ebNm2yHqO4uFher9e/ZWZmtqckAFGA3gHYtXkoNzU1acaMGRoxYoQGDRokSaqqqlJSUpK6devWbN+0tDRVVVVZj1NUVCSfz+ffKisr21oSgChA7wACa/NtNgsLC7Vr1y598MEH7SrA4/HI4/G06xgAoge9AwisTUN52rRpWr16tTZu3Kg+ffr48/T0dJ06dUo1NTXNfuOtrq5Wenp6u4uFu4wx1jxUq0MRG+gd327IkCHW/Ot/1v+6QPe4PnXqlDVfuHChNa+urm65OJwXQXVMY4ymTZumkpISrVu3TllZWc0+P3ToUHXs2FGlpaX+rKKiQgcPHlRubm5oKgYQdegdQOsEdaVcWFioZcuWadWqVUpOTvY/1+P1etW5c2d5vV7dddddmjVrllJTU5WSkqLp06crNzfXunoSQHygdwCtE9RQXrRokSRp9OjRzfLFixdrypQpkqRf/OIXSkxM1KRJk5rdAABA/KJ3AK0T1FAO9Lzh13Xq1EkLFy4M+NwFgPhD7wBah1U4AAA4os0viQJaI9AinSVLlpzfQoAo8M3XaZ8V7Ar0zz77zJo/+OCDwZaE84wrZQAAHMFQBgDAEQxlAAAcwVAGAMARDGUAABzB6muEREJCQqRLAICox5UyAACOYCgDAOAIhjIAAI5gKAMA4AiGMgAAjmD1NYLypz/9yZr/+Mc/Ps+VALFnz5491vzDDz+05iNHjgxnOYgArpQBAHAEQxkAAEcwlAEAcARDGQAARzCUAQBwhQnCM888Y6688kpz4YUXmp49e5px48aZPXv2NNvnmmuuMZKabffcc0+rH8Pn853z9Wxsrm0+ny+YH524R+9gYzuztdQ7grpSLisrU2FhoTZv3qy1a9fq9OnTGjNmjOrr65vtd/fdd+vIkSP+bd68ecE8DIAYQ+8AWieo1ymvWbOm2cdLlixRr169VF5erlGjRvnzLl26KD09vVXHbGhoUENDg//j2traYEoCEAXoHUDrtOs5ZZ/PJ0lKTU1tlr/11lvq0aOHBg0apKKiIn355ZcBj1FcXCyv1+vfMjMz21MSgChA7wDsEowxpi1f2NTUpB/96EeqqanRBx984M//4z/+Q/369VNGRoZ27typRx55RDk5Ofr9739vPY7tt11+uOA6n8+nlJSUSJcRlegdiGct9o42rNkwxhhz7733mn79+pnKyspv3a+0tNRIMvv27WvVcVmswRYNGwu92o7ewRbPW0gXep01bdo0rV69WuvXr1efPn2+dd/hw4dLkvbt29eWhwIQQ+gdwLcLaqGXMUbTp09XSUmJNmzYoKysrBa/ZseOHZKk3r17t6lAANGP3gG0TlBDubCwUMuWLdOqVauUnJysqqoqSZLX61Xnzp21f/9+LVu2TDfccIO6d++unTt3aubMmRo1apQGDx4clhMA4D56B9BKrXwayBhjAv6NfPHixcYYYw4ePGhGjRplUlNTjcfjMZdddpl56KGHgnr+jeeF2KJh4znl4AT6d6R3sMXb1tL3dJtXX4dLbW2tvF5vpMsAvhWrr91D70A0aKl3cO9rAAAcwVAGAMARDGUAABzBUAYAwBEMZQAAHMFQBgDAEc4NZcdeoQVY8X3qHv6fIBq09H3q3FCuq6uLdAlAi/g+dQ//TxANWvo+de7mIU1NTTp8+LCSk5NVV1enzMxMVVZWxsWNGs6+9Rzn6y5jjOrq6pSRkaHEROd+p41r9A7O12Wt7R1B3fv6fEhMTPS/e0xCQoIkKSUlJWr+4UOB83Ubd41yE72D83Vda3oHv+oDAOAIhjIAAI5weih7PB7NnTtXHo8n0qWcF5wvEBrx9r3F+cYO5xZ6AQAQr5y+UgYAIJ4wlAEAcARDGQAARzCUAQBwBEMZAABHOD2UFy5cqIsvvlidOnXS8OHD9de//jXSJYXExo0bddNNNykjI0MJCQlauXJls88bYzRnzhz17t1bnTt3Vl5envbu3RuZYkOguLhYw4YNU3Jysnr16qXx48eroqKi2T4nT55UYWGhunfvrgsvvFCTJk1SdXV1hCpGNIvVviHFV++I177h7FB+5513NGvWLM2dO1fbtm1Tdna28vPzdfTo0UiX1m719fXKzs7WwoULrZ+fN2+eFixYoFdeeUVbtmxR165dlZ+fr5MnT57nSkOjrKxMhYWF2rx5s9auXavTp09rzJgxqq+v9+8zc+ZMvffee1qxYoXKysp0+PBhTZw4MYJVIxrFct+Q4qt3xG3fMI7KyckxhYWF/o8bGxtNRkaGKS4ujmBVoSfJlJSU+D9uamoy6enpZv78+f6spqbGeDwe8/bbb0egwtA7evSokWTKysqMMWfOr2PHjmbFihX+fT755BMjyWzatClSZSIKxUvfMCb+eke89A0nr5RPnTql8vJy5eXl+bPExETl5eVp06ZNEaws/A4cOKCqqqpm5+71ejV8+PCYOXefzydJSk1NlSSVl5fr9OnTzc554MCB6tu3b8ycM8IvnvuGFPu9I176hpND+YsvvlBjY6PS0tKa5WlpaaqqqopQVefH2fOL1XNvamrSjBkzNGLECA0aNEjSmXNOSkpSt27dmu0bK+eM8yOe+4YU270jnvqGc2/diNhWWFioXbt26YMPPoh0KQCiRDz1DSevlHv06KEOHTqcs4quurpa6enpEarq/Dh7frF47tOmTdPq1au1fv16//veSmfO+dSpU6qpqWm2fyycM86feO4bUuz2jnjrG04O5aSkJA0dOlSlpaX+rKmpSaWlpcrNzY1gZeGXlZWl9PT0ZudeW1urLVu2RO25G2M0bdo0lZSUaN26dcrKymr2+aFDh6pjx47NzrmiokIHDx6M2nPG+RfPfUOKvd4Rt30j0ivNAlm+fLnxeDxmyZIlZvfu3Wbq1KmmW7dupqqqKtKltVtdXZ3Zvn272b59u5FkXnjhBbN9+3bz6aefGmOMefbZZ023bt3MqlWrzM6dO824ceNMVlaWOXHiRIQrb5v77rvPeL1es2HDBnPkyBH/9uWXX/r3uffee03fvn3NunXrzNatW01ubq7Jzc2NYNWIRrHcN4yJr94Rr33D2aFsjDEvvfSS6du3r0lKSjI5OTlm8+bNkS4pJNavX28knbNNnjzZGHPmpQ2PPfaYSUtLMx6Px1x//fWmoqIiskW3g+1cJZnFixf79zlx4oS5//77zUUXXWS6dOliJkyYYI4cORK5ohG1YrVvGBNfvSNe+wbvpwwAgCOcfE4ZAIB4xFAGAMARDGUAABzBUAYAwBEMZQAAHMFQBgDAEQxlAAAcwVAGAMARDGUAABzBUAYAwBEMZQAAHPH/oCZaGIrRfs4AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TBa88B6nDe1"
      },
      "source": [
        "---\n",
        "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie0joAaKnDe1"
      },
      "source": [
        "<a id=\"section3\"></a>\n",
        "# <font color=\"#004D7F\" size=6>3. MLP de línea base</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKcntOOonDe1"
      },
      "source": [
        "Vamos a usar un MLP clásico como base para la comparación con modelos de redes neuronales convolucionales.\n",
        "\n",
        "Importamos las clases, funciones y el dataset MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K39Eqp7nDe2"
      },
      "source": [
        "# Baseline MLP for MNIST dataset\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import mnist\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzpxYVxKnDe2"
      },
      "source": [
        "Para un MLP clásico debemos reducir las imágenes a un vector de píxeles. En este caso, las imágenes de tamaño $28 × 28$ serán vectores de entrada de 784 píxeles.\n",
        "\n",
        "Realizamos esta transformación meidante la función `reshape()`.\n",
        "\n",
        "Los valores de los píxeles son números enteros, por lo que los convertimos a punto flotante para poder normalizarlos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tO3bgVETnDe2"
      },
      "source": [
        "# flatten 28*28 images to a 784 vector for each image\n",
        "num_pixeles = X_train.shape[1] * X_train.shape[2]\n",
        "\n",
        "X_train = X_train.reshape((X_train.shape[0], num_pixeles)).astype('float32')\n",
        "X_test = X_test.reshape((X_test.shape[0], num_pixeles)).astype('float32')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "X2VsW7rToulq",
        "outputId": "2029896c-5a0e-492a-cf3a-f48b079bb1a5"
      },
      "source": [
        "X_train.shape[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Dt5Jyh5nDe2"
      },
      "source": [
        "Los valores de los píxeles están en una escala de grises entre 0 y 255. Podemos normalizar los valores de los píxeles en el rango 0 y 1 dividiendo cada valor por el máximo valor, i.e., 255."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DByKdm21nDe3"
      },
      "source": [
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUH7i1WCnDe3"
      },
      "source": [
        "Finalmente, la variable de salida es un número entero de 0 a 9. Por tanto, usaremos One-Hot Encoding para transformar el vector de enteros de clase en una matriz binaria.\n",
        "\n",
        "Usaremos para ello la función de Keras `np_utils.to_categorical()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-R_pVM3nDe3"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# One hot encode outputs\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "num_classes = y_test.shape[1]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qLJ0AsYTrAtN",
        "outputId": "c1a46631-86c9-4637-88f4-58122441a23e"
      },
      "source": [
        "y_test.shape[1]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrFAZu3IQFsD"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWFbBy2pnDe3"
      },
      "source": [
        "Vamos a definir nuestro modelo:\n",
        "1. El número de entradas será el tamaño máximo de pixeles (784)\n",
        "2. Tendrá una capa oculta con el mismo número de neuronas que entradas (784).\n",
        "3. Se utiliza una función de activación ReLU en la capa oculta.\n",
        "4. Se utiliza una función de activación Softmax en la capa de salida.\n",
        "5. La función de pérdida será `categorical_crossentropy`.\n",
        "6. Utilizaremos ADAM para aprender los pesos.\n",
        "<img src=\"https://drive.google.com/uc?id=1swPTlvecinN9uuyl5LO0fKIsxniOkcwq\" width=\"671\" height=\"177\" />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEI33JAXnDe4"
      },
      "source": [
        "# define baseline model\n",
        "def baseline_model():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(num_pixeles, input_dim=num_pixeles, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx1vazGpnDe4"
      },
      "source": [
        "Entrenamos y evaluamos el modelo.\n",
        "1. El modelo se ajusta a más de 10 épocas con actualizaciones cada 200 imágenes.\n",
        "2. Los datos de test se utilizan como conjunto de datos de validación.\n",
        "3. Se utiliza un valor `verbose` de 2.\n",
        "4. Evaluamos en test e imprimimos las métricas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9P89UYPnnDe4",
        "outputId": "7198e664-234b-46f2-cb36-91190c6a8c47"
      },
      "source": [
        "# build the model\n",
        "model = baseline_model()\n",
        "\n",
        "# Fit the model\n",
        "# verbose=0 will show you nothing (silent)\n",
        "# verbose=1 will show you an animated progress bar like this:\n",
        "# verbose=2 will just mention the number of epoch like this: Epoch 1/10\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Error del modelo de la linea base: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "300/300 - 7s - 25ms/step - accuracy: 0.9216 - loss: 0.2781 - val_accuracy: 0.9612 - val_loss: 0.1354\n",
            "Epoch 2/10\n",
            "300/300 - 5s - 17ms/step - accuracy: 0.9678 - loss: 0.1105 - val_accuracy: 0.9723 - val_loss: 0.0947\n",
            "Epoch 3/10\n",
            "300/300 - 7s - 22ms/step - accuracy: 0.9794 - loss: 0.0703 - val_accuracy: 0.9711 - val_loss: 0.0881\n",
            "Epoch 4/10\n",
            "300/300 - 5s - 17ms/step - accuracy: 0.9856 - loss: 0.0500 - val_accuracy: 0.9779 - val_loss: 0.0726\n",
            "Epoch 5/10\n",
            "300/300 - 6s - 20ms/step - accuracy: 0.9901 - loss: 0.0358 - val_accuracy: 0.9791 - val_loss: 0.0654\n",
            "Epoch 6/10\n",
            "300/300 - 10s - 32ms/step - accuracy: 0.9932 - loss: 0.0261 - val_accuracy: 0.9807 - val_loss: 0.0626\n",
            "Epoch 7/10\n",
            "300/300 - 6s - 21ms/step - accuracy: 0.9956 - loss: 0.0188 - val_accuracy: 0.9792 - val_loss: 0.0673\n",
            "Epoch 8/10\n",
            "300/300 - 5s - 18ms/step - accuracy: 0.9966 - loss: 0.0146 - val_accuracy: 0.9815 - val_loss: 0.0607\n",
            "Epoch 9/10\n",
            "300/300 - 6s - 21ms/step - accuracy: 0.9975 - loss: 0.0110 - val_accuracy: 0.9812 - val_loss: 0.0634\n",
            "Epoch 10/10\n",
            "300/300 - 9s - 31ms/step - accuracy: 0.9984 - loss: 0.0084 - val_accuracy: 0.9815 - val_loss: 0.0605\n",
            "Error del modelo de la linea base: 1.85%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H86uDB8YnDe5"
      },
      "source": [
        "---\n",
        "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho7MyE08nDe5"
      },
      "source": [
        "<a id=\"section4\"></a>\n",
        "# <font color=\"#004D7F\" size=6>4. CNN para MNIST </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSdsGfWbnDe5"
      },
      "source": [
        "Ahora que hemos visto cómo cargar el conjunto de datos MNIST y entrenar un modelo simple de perceptrón multicapa en él, es hora de desarrollar una red neuronal convolucional más sofisticada o un modelo CNN.\n",
        "\n",
        "Crearemos una CNN simple para MNIST que demuestra cómo utilizar todos los aspectos de una implementación de CNN moderna, incluidas las capas convolucionales, las capas de agrupación y las capas de dropout.\n",
        "\n",
        "El primer paso es importar las clases y funciones necesarias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQRwF3kYnDe5"
      },
      "source": [
        "# Simple CNN for the MNIST Dataset ( Cambie a la version mas reciente con tensorflow por probelmas de compatibildiad )\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Mdqwqd1nDe6"
      },
      "source": [
        "En Keras, las capas utilizadas para convoluciones bidimensionales esperan valores de píxeles con las dimensiones `[muestras]-[ancho]-[alto]-[canales]`.\n",
        "\n",
        "En cuanto al canal en MNIST, ya que está dada en escala de grises, la dimensión de píxel se establece en 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jErc683MnDe6"
      },
      "source": [
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# reshape to be [samples][width][height][channels]\n",
        "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float')\n",
        "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float')\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm8FXMvSu8tx"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THgWiB53nDe6"
      },
      "source": [
        "Normalizamos los valores de los píxeles en el rango 0 y 1 y realizar OHE en el target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJFJdOdVnDe6"
      },
      "source": [
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "# one hot encode outputs\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "num_classes = y_test.shape[1]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uLM_JoFnDe7"
      },
      "source": [
        "A continuación, definimos nuestro modelo de red neuronal:\n",
        "\n",
        "1. La primera capa oculta es una capa convolucional llamada `Conv2D`.\n",
        "    * Tiene 32 mapas de características, con un tamaño de $5 × 5$ y una función de activación ReLu.\n",
        "2. Capa Pooling  `MaxPooling2D `.\n",
        "    * Tamaño de pacht de $2 × 2$.\n",
        "3. Capa de regularización `Dropout`.\n",
        "4. Capa `Flatten` para conversión de la matriz 2D en un vector (1D).\n",
        "5. Capa `Dense` con 128 neuronas y la función de activación ReLU.\n",
        "6. Capa de salida con 10 neuronas para las 10 clases y una función de activación **Softmax**.\n",
        "7. La compilación con ADAM, pérdida logarítmica como función de coste y Accuracy como métrica.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1GkKR_mdfuDpH-CxJ0V2vZJ3m63CxM0-Z\" width=\"1487\" height=\"178\" />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYqlikeXnDe7"
      },
      "source": [
        "# define a simple CNN model\n",
        "def baseline_model():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (5,5), input_shape=(28,28,1), activation='relu'))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    return model"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8x0chjEnDe7"
      },
      "source": [
        "Entrenamos con 10 épocas a un tamaño de batch de 200.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CQLwf75NnDe7",
        "outputId": "de34453f-fb34-4f7a-c4f0-9505d73e49de"
      },
      "source": [
        "# build the model\n",
        "model = baseline_model()\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Error del modelo de CNN linea base: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 110ms/step - accuracy: 0.8556 - loss: 0.5149 - val_accuracy: 0.9780 - val_loss: 0.0772\n",
            "Epoch 2/10\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 105ms/step - accuracy: 0.9775 - loss: 0.0780 - val_accuracy: 0.9844 - val_loss: 0.0507\n",
            "Epoch 3/10\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 109ms/step - accuracy: 0.9834 - loss: 0.0536 - val_accuracy: 0.9850 - val_loss: 0.0441\n",
            "Epoch 4/10\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 105ms/step - accuracy: 0.9869 - loss: 0.0429 - val_accuracy: 0.9881 - val_loss: 0.0353\n",
            "Epoch 5/10\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 104ms/step - accuracy: 0.9885 - loss: 0.0349 - val_accuracy: 0.9895 - val_loss: 0.0317\n",
            "Epoch 6/10\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 108ms/step - accuracy: 0.9919 - loss: 0.0268 - val_accuracy: 0.9900 - val_loss: 0.0317\n",
            "Epoch 7/10\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 108ms/step - accuracy: 0.9922 - loss: 0.0233 - val_accuracy: 0.9873 - val_loss: 0.0363\n",
            "Epoch 8/10\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 103ms/step - accuracy: 0.9929 - loss: 0.0212 - val_accuracy: 0.9903 - val_loss: 0.0301\n",
            "Epoch 9/10\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 103ms/step - accuracy: 0.9950 - loss: 0.0165 - val_accuracy: 0.9889 - val_loss: 0.0332\n",
            "Epoch 10/10\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 107ms/step - accuracy: 0.9955 - loss: 0.0150 - val_accuracy: 0.9898 - val_loss: 0.0294\n",
            "Error del modelo de CNN linea base: 1.02%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ31DLgQnDe8"
      },
      "source": [
        "---\n",
        "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyc9tg3knDe8"
      },
      "source": [
        "<a id=\"section5\"></a>\n",
        "# <font color=\"#004D7F\" size=6>5. CNN más profunda para MNIST </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHxVhqc3nDe8"
      },
      "source": [
        "Esta vez definimos una arquitectura con más capas de convolucionales, Max-pooling y capas completamente conectadas.\n",
        "\n",
        "1. Capa convolucional con 30 mapas de tamaño $5 × 5$.\n",
        "2. Capa de Pooling con patch de $2 × 2$.\n",
        "3. Capa convolucional con 15 mapas de tamaño $3 × 3$.\n",
        "4. Capa de Pooling con patch de $2 × 2$.\n",
        "5. Capa de Dropout del 20%.\n",
        "6. Capa Flatten.\n",
        "7. Capa completamente conectada con 128 neuronas y ReLu.\n",
        "8. Capa completamente conectada con 50 neuronas y ReLu\n",
        "9. Capa de salida cpm activación Softmax.\n",
        "10. La compilación con ADAM, pérdida logarítmica como función de coste y Accuracy como métrica.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1j5bxc-9useczn5A1B4gbzKEnJHAklmku\" width=\"2202\" height=\"179\" />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztTgFky2nDe8",
        "outputId": "c47cb405-d85e-41e1-bb17-417564681b6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def larger_model():\n",
        "    # Crear modelo\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(15, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # Compilar modelo\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Construir el modelo\n",
        "model = larger_model()\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
        "\n",
        "# Evaluar el modelo\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Error del modelo CNN profunda: %.2f%%\" % (100 - scores[1] * 100))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "300/300 - 39s - 131ms/step - accuracy: 0.8802 - loss: 0.3881 - val_accuracy: 0.9765 - val_loss: 0.0801\n",
            "Epoch 2/10\n",
            "300/300 - 37s - 122ms/step - accuracy: 0.9693 - loss: 0.0978 - val_accuracy: 0.9817 - val_loss: 0.0541\n",
            "Epoch 3/10\n",
            "300/300 - 35s - 117ms/step - accuracy: 0.9777 - loss: 0.0707 - val_accuracy: 0.9874 - val_loss: 0.0368\n",
            "Epoch 4/10\n",
            "300/300 - 41s - 136ms/step - accuracy: 0.9825 - loss: 0.0557 - val_accuracy: 0.9880 - val_loss: 0.0347\n",
            "Epoch 5/10\n",
            "300/300 - 41s - 135ms/step - accuracy: 0.9847 - loss: 0.0481 - val_accuracy: 0.9901 - val_loss: 0.0282\n",
            "Epoch 6/10\n",
            "300/300 - 39s - 129ms/step - accuracy: 0.9864 - loss: 0.0423 - val_accuracy: 0.9897 - val_loss: 0.0294\n",
            "Epoch 7/10\n",
            "300/300 - 42s - 141ms/step - accuracy: 0.9879 - loss: 0.0394 - val_accuracy: 0.9909 - val_loss: 0.0260\n",
            "Epoch 8/10\n",
            "300/300 - 42s - 141ms/step - accuracy: 0.9884 - loss: 0.0351 - val_accuracy: 0.9917 - val_loss: 0.0250\n",
            "Epoch 9/10\n",
            "300/300 - 40s - 132ms/step - accuracy: 0.9897 - loss: 0.0321 - val_accuracy: 0.9916 - val_loss: 0.0257\n",
            "Epoch 10/10\n",
            "300/300 - 40s - 135ms/step - accuracy: 0.9907 - loss: 0.0282 - val_accuracy: 0.9914 - val_loss: 0.0254\n",
            "Error del modelo CNN profunda: 0.86%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk8eZ6eqnDe9"
      },
      "source": [
        "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
        "\n",
        "---\n",
        "\n",
        "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-coffee\" aria-hidden=\"true\" style=\"color:#004D7F\"></i> </font></div>"
      ]
    }
  ]
}